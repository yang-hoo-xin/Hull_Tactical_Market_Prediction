{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yang25/ml-p4?scriptVersionId=283856293\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"2785b804","metadata":{"papermill":{"duration":0.002189,"end_time":"2025-12-04T12:55:32.291024","exception":false,"start_time":"2025-12-04T12:55:32.288835","status":"completed"},"tags":[]},"source":["**Baseline model**"]},{"cell_type":"markdown","id":"d24174ac","metadata":{"papermill":{"duration":0.001533,"end_time":"2025-12-04T12:55:32.294081","exception":false,"start_time":"2025-12-04T12:55:32.292548","status":"completed"},"tags":[]},"source":["**Integrated improved model**"]},{"cell_type":"code","execution_count":1,"id":"722cea4c","metadata":{"execution":{"iopub.execute_input":"2025-12-04T12:55:32.298334Z","iopub.status.busy":"2025-12-04T12:55:32.298102Z","iopub.status.idle":"2025-12-04T12:56:40.29308Z","shell.execute_reply":"2025-12-04T12:56:40.292038Z"},"papermill":{"duration":67.999011,"end_time":"2025-12-04T12:56:40.294498","exception":false,"start_time":"2025-12-04T12:55:32.295487","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Attempting to load Kaggle train.csv...\n","Applying Long-Only target transformation: target = max(0, target)\n","Loaded 9021 rows. Using 22 features (V/D columns).\n","\n","Performing time-series Feature Engineering...\n","New feature set size: 30 columns.\n","Generated 5 walk-forward splits.\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000392 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 5257\n","[LightGBM] [Info] Number of data points in the train set: 1189, number of used features: 30\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Start training from score 0.002976\n","Training until validation scores don't improve for 20 rounds\n","Early stopping, best iteration is:\n","[24]\tvalid_0's l2: 3.35804e-05\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000429 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 5343\n","[LightGBM] [Info] Number of data points in the train set: 1426, number of used features: 30\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Start training from score 0.003106\n","Training until validation scores don't improve for 20 rounds\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Early stopping, best iteration is:\n","[140]\tvalid_0's l2: 6.83169e-05\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000457 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 5347\n","[LightGBM] [Info] Number of data points in the train set: 1663, number of used features: 30\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Start training from score 0.003487\n","Training until validation scores don't improve for 20 rounds\n","Early stopping, best iteration is:\n","[26]\tvalid_0's l2: 3.05646e-05\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000449 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 5353\n","[LightGBM] [Info] Number of data points in the train set: 1900, number of used features: 30\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Start training from score 0.003583\n","Training until validation scores don't improve for 20 rounds\n","Early stopping, best iteration is:\n","[84]\tvalid_0's l2: 4.64764e-05\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000459 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 5353\n","[LightGBM] [Info] Number of data points in the train set: 2137, number of used features: 30\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Start training from score 0.003690\n","Training until validation scores don't improve for 20 rounds\n","Early stopping, best iteration is:\n","[79]\tvalid_0's l2: 5.87255e-05\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","Training final base models on full training set...\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000543 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 5357\n","[LightGBM] [Info] Number of data points in the train set: 2378, number of used features: 30\n","[LightGBM] [Info] Start training from score 0.003826\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] lambda_l2 is set=0.005, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005\n","[LightGBM] [Warning] lambda_l1 is set=0.005, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005\n","Training Meta-Learner (Ridge)...\n","\n","--- Final Performance on Test Set ---\n","Modified Sharpe: 9.6615\n","Strategy Annualized Return: 0.9425 (Base: 0.8989)\n","Strategy Annualized Volatility: 0.0976\n","\n","Saving results...\n","Submission saved to: /kaggle/working/submission.parquet\n","Feature Importance saved to: /kaggle/working/feature_importances.csv\n","Plot saved to: /kaggle/working/strategy_performance.png\n","\n","Pipeline finished successfully.\n"]}],"source":["import os\n","import math\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import RandomForestRegressor\n","from pathlib import Path\n","\n","# --- 1. 库检查与配置 ---\n","try:\n","    import lightgbm as lgb\n","    LGB_AVAILABLE = True\n","except ImportError:\n","    LGB_AVAILABLE = False\n","    \n","try:\n","    from catboost import CatBoostRegressor\n","    CAT_AVAILABLE = True\n","except ImportError:\n","    CAT_AVAILABLE = False\n","\n","warnings.filterwarnings(\"ignore\")\n","np.random.seed(42)\n","\n","# --- CONFIG ---\n","# 请根据您的实际 Kaggle 文件路径进行修改\n","KAGGLE_INPUT_PATH = \"/kaggle/input/hull-tactical-market-prediction/\" \n","OUTPUT_DIR = \"/kaggle/working\"\n","OUT_SUB = os.path.join(OUTPUT_DIR, \"submission.csv\")\n","OUT_PLOT = os.path.join(OUTPUT_DIR, \"strategy_performance.png\")\n","OUT_FI = os.path.join(OUTPUT_DIR, \"feature_importances.csv\")\n","\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","def log(msg):\n","    \"\"\"简单的日志输出函数\"\"\"\n","    print(msg)\n","\n","# --- 2. 评估指标模块 ---\n","\n","def sharpe_like(alloc, realized_returns, market_returns, risk_free_rate=0.02):\n","    \"\"\"\n","    计算修正夏普比率 (Modified Sharpe Ratio)，包含波动率和跑输惩罚。\n","    符合 Project 4 的评估要求。\n","    \"\"\"\n","    if len(alloc) == 0:\n","        return 0.0, 0.0, 0.0, 0.0\n","\n","    # 1. 策略超额收益\n","    strat_ret_excess = (alloc * realized_returns) # 假设 realized_returns 已经是超额收益\n","\n","    # 2. 年化表现 (假设 252 个交易日)\n","    ann_ret_excess = np.mean(strat_ret_excess) * 252\n","    ann_vol = np.std(strat_ret_excess) * math.sqrt(252)\n","\n","    # 3. 基准表现\n","    base_ann_ret_excess = np.mean(market_returns) * 252\n","    market_vol = np.std(market_returns) * math.sqrt(252)\n","    \n","    # 4. 波动率惩罚 (目标: ann_vol <= 1.2 * market_vol)\n","    vol_pen = 1.0\n","    TARGET_VOL_MAX = 1.2 * market_vol\n","    if ann_vol > TARGET_VOL_MAX:\n","        # 超出 120% 波动率的惩罚因子\n","        vol_pen = 1.0 - min(0.9, (ann_vol / TARGET_VOL_MAX - 1.0))\n","    \n","    # 5. 表现惩罚 (如果跑输市场，夏普比率打折)\n","    perf_pen = 1.0 if ann_ret_excess >= base_ann_ret_excess else 0.5\n","    \n","    # 6. 计算最终的修正夏普比率\n","    sharpe = ann_ret_excess / (ann_vol + 1e-9)\n","    modified_sharpe = sharpe * vol_pen * perf_pen\n","    \n","    return modified_sharpe, ann_ret_excess, ann_vol, base_ann_ret_excess\n","\n","# --- 3. 风险管理模块 (波动率目标分配) ---\n","\n","def vol_target_allocation(raw_pred, realized_returns, market_returns, target_vol_scale=1.2, max_leverage=2.0):\n","    \"\"\"\n","    将原始预测信号转换为满足波动率约束的最终仓位。\n","    \"\"\"\n","    \n","    # 1. 平滑信号 (使用 5 日移动平均以增加鲁棒性)\n","    signal = pd.Series(raw_pred).rolling(5, min_periods=1).mean().values\n","    \n","    # 2. 市场波动率 (用于设定目标上限)\n","    market_vol_annual = np.std(market_returns) * math.sqrt(252) if len(market_returns) > 1 else 0.15\n","    \n","    # 3. 策略实际波动率 (用于动态缩放信号)\n","    # 这里的估算需要改进，但我们先用策略实现的年化波动率来代理\n","    realized_vol_annual = np.std(signal * realized_returns) * math.sqrt(252) if len(realized_returns) > 1 else 0.10\n","    \n","    # 4. 目标波动率 (120% 约束)\n","    target_vol = target_vol_scale * market_vol_annual\n","    \n","    # 5. 缩放因子 (如果波动率太低就放大，太高就缩小)\n","    scale = target_vol / (realized_vol_annual + 1e-9)\n","    \n","    # 6. 最终仓位 (Long-Only [0.0, 2.0] 是上一个代码的假设，如果允许 Short，需要改成 [-2.0, 2.0])\n","    pos = signal * scale\n","    \n","    # 强制执行最大杠杆和 Long-Only (根据原始参考代码意图)\n","    pos = np.clip(pos, 0.0, max_leverage)\n","    \n","    return pos\n","\n","# --- 4. 数据准备与验证划分模块 ---\n","\n","def load_kaggle_data(path=KAGGLE_INPUT_PATH):\n","    \"\"\"加载 Kaggle 竞赛数据并准备特征列。\"\"\"\n","    log(\"Attempting to load Kaggle train.csv...\")\n","    file_path = os.path.join(path, \"train.csv\")\n","    \n","    # 尝试在 /kaggle/input/ 目录下查找文件（常见于 Kaggle 环境）\n","    try:\n","        df = pd.read_csv(file_path)\n","    except FileNotFoundError:\n","        log(f\"Error: train.csv not found at {file_path}. Please check your Kaggle path.\")\n","        raise\n","\n","    # 1. 重命名目标列\n","    if 'forward_returns' in df.columns:\n","        df = df.rename(columns={'forward_returns': 'target'})\n","    \n","    # 2. 使用 'date_id' 作为时间索引\n","    if 'date_id' in df.columns:\n","        df['date'] = df['date_id']\n","    \n","    # 3. 过滤 NaN 目标值\n","    df = df.dropna(subset=['target']).reset_index(drop=True)\n","\n","    log(\"Applying Long-Only target transformation: target = max(0, target)\")\n","    df['target'] = np.maximum(0, df['target'])\n","    \n","    # 4. 定义特征列 (V-cols and D-cols)\n","    V_cols = [c for c in df.columns if c.startswith('V') and c[1:].isdigit()]\n","    D_cols = [c for c in df.columns if c.startswith('D') and c[1:].isdigit()]\n","    \n","    feat_cols = V_cols + D_cols\n","    \n","    log(f\"Loaded {len(df)} rows. Using {len(feat_cols)} features (V/D columns).\")\n","    \n","    return df, feat_cols\n","\n","def rolling_walk_forward_splits(n, n_splits=5, min_train_ratio=0.5):\n","    \"\"\"\n","    生成时间序列滚动向前交叉验证 (Walk-Forward CV) 划分。\n","    \"\"\"\n","    min_train_size = int(n * min_train_ratio)\n","    splits = []\n","    test_size = max(1, (n - min_train_size) // n_splits)\n","    start_train = 0\n","    \n","    for i in range(n_splits):\n","        train_end = min_train_size + i * test_size\n","        val_start = train_end\n","        val_end = min(val_start + test_size, n)\n","        \n","        if val_end - val_start <= 0 or val_start >= n: break\n","            \n","        train_idx = list(range(start_train, val_start))\n","        val_idx = list(range(val_start, val_end))\n","        \n","        splits.append((train_idx, val_idx))\n","    \n","    log(f\"Generated {len(splits)} walk-forward splits.\")\n","    return splits\n","\n","# --- 5. 模型训练与集成模块 ---\n","\n","def train_and_predict(df_model, feat_cols, target_col='target', date_col='date'):\n","    \n","    # 1. 划分 Full Train (用于 OOF 和 Meta-Learner) / Final Test (用于最终评估)\n","    df_model = df_model.sort_values(date_col).reset_index(drop=True)\n","    unique_dates = df_model[date_col].sort_values().unique()\n","    split_date_idx = int(len(unique_dates) * 0.8) # 80% 用于训练/CV\n","    split_date = unique_dates[split_date_idx]\n","    \n","    train_full = df_model[df_model[date_col] <= split_date].reset_index(drop=True)\n","    test_final = df_model[df_model[date_col] > split_date].reset_index(drop=True)\n","    \n","    X_tr_full = train_full[feat_cols].values; y_tr_full = train_full[target_col].values\n","    X_te_final = test_final[feat_cols].values; y_te_final = test_final[target_col].values\n","    \n","    # 2. Walk-Forward CV 设置\n","    splits = rolling_walk_forward_splits(len(train_full), n_splits=5)\n","    oof_lgb = np.zeros(len(train_full)); oof_count = np.zeros(len(train_full))\n","    oof_cat = np.zeros(len(train_full)) if CAT_AVAILABLE else None\n","\n","  # 模型参数 (可根据需要调整)\n","    lgb_params = {\n","        'n_estimators': 600,\n","        'learning_rate': 0.015,         # 学习率略微降低，以提高稳定性\n","        'max_depth': 8,                 # 恢复到更稳定的深度\n","        'min_child_samples': 10,        # 恢复到更稳定的叶子节点数\n","        'lambda_l1': 0.005,             # 极少量正则化\n","        'lambda_l2': 0.005,             # 极少量正则化\n","        'random_state': 42, \n","        'n_jobs': -1\n","    }\n","\n","    cat_params = {'iterations': 300, 'learning_rate': 0.01, 'depth': 10, 'l2_leaf_reg': 1.0, 'random_state': 42, 'verbose': 0}\n","\n","    # 3. 训练基模型并生成 OOF (Out-of-Fold) 预测\n","    for fold, (tr_idx, val_idx) in enumerate(splits):\n","        Xtr = train_full.iloc[tr_idx][feat_cols].values; ytr = train_full.iloc[tr_idx][target_col].values\n","        Xval = train_full.iloc[val_idx][feat_cols].values; yval = train_full.iloc[val_idx][target_col].values\n","        \n","        # Base Model 1: LightGBM\n","        if LGB_AVAILABLE:\n","            m_lgb = lgb.LGBMRegressor(**lgb_params)\n","            m_lgb.fit(Xtr, ytr, eval_set=[(Xval, yval)], callbacks=[lgb.early_stopping(20, False), lgb.log_evaluation(0)])\n","            oof_lgb[val_idx] = m_lgb.predict(Xval)\n","        else: # Fallback to Random Forest\n","            m_rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42).fit(Xtr, ytr)\n","            oof_lgb[val_idx] = m_rf.predict(Xval)\n","\n","        # Base Model 2: CatBoost\n","        if CAT_AVAILABLE:\n","            m_cat = CatBoostRegressor(**cat_params)\n","            m_cat.fit(Xtr, ytr, eval_set=(Xval, yval), use_best_model=True)\n","            oof_cat[val_idx] = m_cat.predict(Xval)\n","            \n","        oof_count[val_idx] += 1\n","        \n","    # 4. 训练最终基模型 (用于生成 Test 预测)\n","    log(\"Training final base models on full training set...\")\n","    \n","    if LGB_AVAILABLE:\n","        final_lgb = lgb.LGBMRegressor(**lgb_params).fit(X_tr_full, y_tr_full)\n","        pred_lgb = final_lgb.predict(X_te_final)\n","        fi_lgb = pd.Series(final_lgb.feature_importances_, index=feat_cols)\n","    else:\n","        final_rf = RandomForestRegressor(n_estimators=300, max_depth=8, random_state=42).fit(X_tr_full, y_tr_full)\n","        pred_lgb = final_rf.predict(X_te_final)\n","        fi_lgb = pd.Series(0, index=feat_cols) # 随机森林也可以导出特征重要性\n","\n","    if CAT_AVAILABLE:\n","        final_cat = CatBoostRegressor(**cat_params).fit(X_tr_full, y_tr_full)\n","        pred_cat = final_cat.predict(X_te_final)\n","    else:\n","        pred_cat = None\n","\n","    # 5. 训练 Meta-Learner (Ridge)\n","    mask = oof_count > 0\n","    stack_train = pd.DataFrame({'lgb': oof_lgb[mask]})\n","    if CAT_AVAILABLE: stack_train['cat'] = oof_cat[mask]\n","        \n","    stack_test = pd.DataFrame({'lgb': pred_lgb})\n","    if CAT_AVAILABLE: stack_test['cat'] = pred_cat\n","\n","    log(\"Training Meta-Learner (Ridge)...\")\n","    meta = Ridge(alpha=1.0, random_state=42).fit(stack_train.values, y_tr_full[mask])\n","    meta_pred = meta.predict(stack_test.values)\n","\n","    # 6. 最终集成预测\n","    if CAT_AVAILABLE:\n","        # 加权平均: LGB(35%) + CAT(35%) + Meta(30%)\n","        final_pred = 0.35 * pred_lgb + 0.35 * pred_cat + 0.3 * meta_pred\n","    else:\n","        # 加权平均: LGB/RF(60%) + Meta(40%)\n","        final_pred = 0.6 * pred_lgb + 0.4 * meta_pred\n","\n","    # 7. 应用风险管理 (波动率目标分配)\n","    market_returns_test = test_final[target_col].values # 使用测试集的超额收益作为基准\n","    alloc = vol_target_allocation(final_pred, y_te_final, market_returns_test, target_vol_scale=1.2, max_leverage=2.0)\n","    \n","    # 8. 评估性能\n","    score, ann_ret, ann_vol, base_ret = sharpe_like(alloc, y_te_final, market_returns_test)\n","    \n","    log(f\"\\n--- Final Performance on Test Set ---\")\n","    log(f\"Modified Sharpe: {score:.4f}\")\n","    log(f\"Strategy Annualized Return: {ann_ret:.4f} (Base: {base_ret:.4f})\")\n","    log(f\"Strategy Annualized Volatility: {ann_vol:.4f}\")\n","\n","    # 9. 准备输出\n","    out_df = pd.DataFrame({\n","        'date_id': test_final[date_col].values,\n","        'allocation': alloc\n","    })\n","    \n","    return out_df, fi_lgb, final_pred, y_te_final\n","\n","# --- 6. 主执行模块 ---\n","\n","if __name__ == \"__main__\":\n","    \n","    # 1. 加载数据\n","    try:\n","        merged_df, features = load_kaggle_data()\n","    except Exception as e:\n","        log(\"Data loading failed. Please check the KAGGLE_INPUT_PATH.\")\n","        raise\n","\n","# === START: 新增特征工程 ===\n","    log(\"\\nPerforming time-series Feature Engineering...\")\n","    \n","    # 1. 滚动平均和标准差 (捕捉动量和波动率)\n","    # 我们对所有的 V/D 特征列进行操作\n","    \n","    for window in [5, 10, 20]: # 5日，10日，20日窗口\n","        merged_df[f'V_mean_{window}'] = merged_df[features].rolling(window=window).mean().mean(axis=1)\n","        merged_df[f'V_std_{window}'] = merged_df[features].rolling(window=window).std().mean(axis=1)\n","\n","    # 2. 目标/市场返回的滞后特征 (最重要的时间信号)\n","    # 滞后 1 天和 5 天的市场超额收益（target）\n","    merged_df['target_lag_1'] = merged_df['target'].shift(1)\n","    merged_df['target_lag_5'] = merged_df['target'].shift(5)\n","\n","    # 3. 目标特征的波动率 (捕捉风险变化)\n","    merged_df['target_vol_5'] = merged_df['target'].shift(1).rolling(window=5).std()\n","\n","    # 4. 更新特征列表\n","    new_features = [c for c in merged_df.columns if c.startswith(('V_', 'target_lag'))]\n","    features += new_features\n","    \n","    # 清理 NaN 值 (滚动特征会生成 NaN，直接丢弃这些行)\n","    merged_df = merged_df.dropna(subset=['target'] + features).reset_index(drop=True)\n","    log(f\"New feature set size: {len(features)} columns.\")\n","    # === END: 新增特征工程 ===\n","    \n","    # 2. 训练、预测、分配\n","    prediction_df, feature_importance, raw_preds, actual_returns = \\\n","        train_and_predict(merged_df, features, target_col='target', date_col='date')\n","\n","    # 3. 保存结果\n","    log(\"\\nSaving results...\")\n","    \n","    # 预测结果 (用于提交)\n","    prediction_df['allocation'] = prediction_df['allocation'].astype(np.float32)\n","    OUT_PARQUET = os.path.join(OUTPUT_DIR, \"submission.parquet\") \n","    prediction_df.to_parquet(OUT_PARQUET, index=False)\n","\n","    try:\n","        del merged_df, features, raw_preds\n","    except NameError:\n","        pass # 如果变量不存在则忽略\n","    \n","    # 特征重要性 (用于报告分析)\n","    feature_importance.to_csv(OUT_FI)\n","\n","    # 绘制结果图 (用于报告分析)\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(prediction_df['date_id'], actual_returns.cumsum(), label='Actual Cumulative Return', color='gray', alpha=0.6)\n","    plt.plot(prediction_df['date_id'], (prediction_df['allocation'] * actual_returns).cumsum(), label='Strategy Cumulative Return', color='red')\n","    plt.title('Cumulative Strategy Return vs. Actual Return')\n","    plt.legend()\n","    plt.savefig(OUT_PLOT)\n","    plt.close()\n","    try:\n","        # 清理绘图后不再需要的变量\n","        del prediction_df, actual_returns, feature_importance\n","    except NameError:\n","        pass\n","    log(f\"Submission saved to: {OUT_PARQUET}\")\n","    log(f\"Feature Importance saved to: {OUT_FI}\")\n","    log(f\"Plot saved to: {OUT_PLOT}\")\n","    log(\"\\nPipeline finished successfully.\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":14348714,"sourceId":111543,"sourceType":"competition"}],"dockerImageVersionId":31193,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":73.917493,"end_time":"2025-12-04T12:56:41.115126","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-04T12:55:27.197633","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}